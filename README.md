ğŸ§  Amazon ML Challenge 2025 â€“ Smart Product Pricing (Multimodal Price Prediction)

A comprehensive machine learning pipeline developed for the Amazon ML Challenge 2025, focused on predicting e-commerce product prices using a multimodal dataset (text + numerical + image features).
This repository contains all code, data-handling scripts, and our final ensemble-based approach achieving a 57.77 SMAPE on the leaderboard.

â¸»

ğŸš€ Project Overview

The challenge required predicting product prices from multiple data types:
	â€¢	ğŸ“ Text features: product titles + descriptions
	â€¢	ğŸ”¢ Numerical features: specs, dimensions, ratings
	â€¢	ğŸ·ï¸ Categorical features: brand, category, subcategory
	â€¢	ğŸ–¼ï¸ Image features: extracted using ResNet (when available)

Even with significant data challenges (â‰ˆ 99.8 % missing images, high price variance), the solution emphasizes:
	â€¢	Robust preprocessing
	â€¢	Advanced feature engineering
	â€¢	Model ensembling
	â€¢	Efficient memory & computation management

â¸»

ğŸ§© Solution Architecture

1ï¸âƒ£ Feature Engineering Pipeline

Component	Techniques Used
Text Processing	TF-IDF vectorization of titles & descriptions
Numerical Features	Log/sqrt transforms, polynomial features
Image Features	ResNet-based embeddings (zero-filled when missing)
Categorical Handling	Target encoding, one-hot for frequent brands/categories
Scaling	RobustScaler to handle outliers
Imputation	Median/mode + category-based defaults


â¸»

2ï¸âƒ£ Advanced Feature Creation
	â€¢	Price interaction ratios and polynomial combinations
	â€¢	K-Means clustering for similarity-based grouping (k = 5, 10, 20)
	â€¢	PCA components to reduce high-dimensional features
	â€¢	Per-category statistics (mean, std, median price)
	â€¢	Target encoding with smoothing

â¸»

3ï¸âƒ£ Model Ensemble Strategy

A weighted ensemble of three gradient-boosting regressors:

Model	Key Config	Notes
ğŸ§  XGBoost	1000 trees, lr = 0.05, max_depth = 8	Main baseline
âš¡ LightGBM	1000 trees, lr = 0.05, boosting_type = â€˜gbdtâ€™	Faster convergence
ğŸˆ CatBoost	1000 iters, lr = 0.05, depth = 8	Best single-model performer

Ensemble weighting: inverse SMAPE per model
Final post-processing: clipping (0.01 â‰¤ price â‰¤ 50 000) + rounding

â¸»

ğŸ“Š Cross-Validation and Evaluation
	â€¢	7-fold cross-validation (stratified by price bins)
	â€¢	Metric: Symmetric Mean Absolute Percentage Error (SMAPE)
	â€¢	Validation consistency: common folds across all models

Final Leaderboard SMAPE: 57.77 %

â¸»

âš™ï¸ Implementation Details

ğŸ§® Feature Selection
	1.	Mutual Information â†’ Top 300 features
	2.	F-Score selection â†’ Refine to 250 most informative
	3.	Correlation filter â†’ drop > 0.95 correlated features

âš¡ Performance Optimizations
	â€¢	Memory-optimized datatypes (float32)
	â€¢	Batch processing & feature caching
	â€¢	Parallelized feature extraction
	â€¢	Early stopping + grid search hyperparameter tuning

â¸»

ğŸ“ Repository Structure

amazon-ml-2025/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __main__.py          # Main training entry
â”‚   â”œâ”€â”€ data.py              # Data loading, preprocessing
â”‚   â”œâ”€â”€ features.py          # Feature engineering utilities
â”‚   â”œâ”€â”€ train_ultra.py       # Model training pipeline
â”‚   â”œâ”€â”€ utils.py             # Helper functions
â”‚   â””â”€â”€ config.py            # Configuration settings
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_submission.py # Final CSV generation
â”‚   â”œâ”€â”€ validate.py          # Validation analysis
â”‚   â””â”€â”€ quick_submission.py  # Rapid local testing
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ train.csv / test.csv
â”‚   â”œâ”€â”€ sample_test.csv
â”‚   â””â”€â”€ images/
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ checkpoints/ logs/ validation/
â””â”€â”€ submissions/


â¸»

ğŸ§  Key Learnings

ğŸ’¡ From this project I learned to:
	â€¢	Systematically analyze and decompose a real-world ML problem
	â€¢	Design a complete workflow â€” from EDA to model validation
	â€¢	Handle multimodal and imbalanced data
	â€¢	Identify and fix bottlenecks during training
	â€¢	Build ensemble systems that generalize better

â¸»

ğŸ¤ Acknowledgments

Big thanks to my teammates for their support throughout the hackathon,
and to the Amazon ML Challenge 2025 organizers for creating such a learning-rich experience!

â¸»

ğŸ§° Tech Stack
	â€¢	Python 3.9 + Pandas + NumPy + Scikit-learn
	â€¢	XGBoost | LightGBM | CatBoost
	â€¢	Matplotlib / Seaborn (for analysis)
	â€¢	PyTorch (ResNet image embeddings)

â¸»

ğŸ“œ License

This project is released under the MIT License.

â¸»
